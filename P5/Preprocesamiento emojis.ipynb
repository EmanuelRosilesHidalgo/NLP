{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9037c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adceaa",
   "metadata": {},
   "source": [
    "<h2> Lematizaci√≥n <h/2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a59b8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     processed_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc])\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processed_text\n\u001b[1;32m----> 6\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes_core_news_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Proceso de tokenizacion y lematizacion  -----------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRest_Mex_2022.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Program Files\\Python39\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program Files\\Python39\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "def lematizacion(text):\n",
    "    doc = nlp(text)\n",
    "    processed_text = ' '.join([token.lemma_ for token in doc])\n",
    "    return processed_text\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Proceso de tokenizacion y lematizacion  -----------------------------------------------------------------\n",
    "\n",
    "df = pd.read_excel(\"Rest_Mex_2022.xlsx\")\n",
    "\n",
    "print(df)\n",
    "\n",
    "df['Title_Opinion'] = df.apply(lambda row: f\"{row['Title']} {row['Opinion']}\", axis=1)\n",
    "df = df.drop(['Title', 'Opinion'], axis=1)\n",
    "\n",
    "df['Title_Opinion'] = df['Title_Opinion'].apply(lematizacion)\n",
    "\n",
    "df.to_pickle('df_tokenizacion_lematizacion_final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43a6123",
   "metadata": {},
   "source": [
    "<h2> Generaci√≥n del lexicon de emojis <h/2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d72e55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        2         11        12\n",
      "0    emoji  negative  positive\n",
      "1        üòÄ  0.356135  0.976363\n",
      "2        üòÉ  0.385544  0.991735\n",
      "3        üòÑ  0.386087  0.885428\n",
      "4        üòÅ   0.37123  0.914501\n",
      "..     ...       ...       ...\n",
      "355      üîµ  0.415303  1.015403\n",
      "356      ‚ö´  0.400755  0.815105\n",
      "357      ‚ö™  0.390018  0.947283\n",
      "358      üèÅ  0.351811  0.985101\n",
      "359   üè≥Ô∏è‚Äçüåà  0.510269  1.481636\n",
      "\n",
      "[360 rows x 3 columns]\n",
      "DataFrame exportado como lexicon_emojis.txt\n"
     ]
    }
   ],
   "source": [
    "# Especifica el nombre del archivo de Excel\n",
    "nombre_archivo = 'Emojis lexicon.xlsx'\n",
    "\n",
    "# Lee el archivo de Excel y crea un DataFrame\n",
    "dataframe = pd.read_excel(nombre_archivo, header=None)  # No es necesario especificar sep='\\t' para archivos de Excel\n",
    "\n",
    "# Selecciona solo las columnas 2, 11 y 12 del DataFrame original\n",
    "df_recortado = dataframe.iloc[:, [2, 11, 12]]\n",
    "\n",
    "# Elimina la primera fila del DataFrame original\n",
    "df = df.drop(df.index[0])\n",
    "\n",
    "# Imprime el DataFrame recortado\n",
    "print(df_recortado)\n",
    "\n",
    "# Guarda el DataFrame recortado en un nuevo archivo de texto\n",
    "df_recortado.to_csv(\"lexicon_emojis.txt\", sep='\\t', index=False)  # Ajusta el delimitador seg√∫n tus necesidades\n",
    "\n",
    "print(f\"DataFrame exportado como lexicon_emojis.txt\")\n",
    "\n",
    "# Lee el archivo CSV especificando los nombres de las columnas\n",
    "df = pd.read_csv('lexicon_emojis.txt', sep='\\t', header=None, names=['emoji', 'negative', 'positive'])\n",
    "\n",
    "# Elimina los duplicados basados en la columna 'emoji'\n",
    "df_sin_duplicados = df.drop_duplicates('emoji')\n",
    "\n",
    "# Crear un diccionario donde cada emoji tenga asociadas listas de tuplas (polaridad, valor)\n",
    "diccionario_caracteristicas = {}\n",
    "\n",
    "# Itera sobre las filas del DataFrame sin duplicados\n",
    "for indice, fila in df_sin_duplicados.iterrows():\n",
    "    emoji = fila['emoji']\n",
    "    tupla_negative = ('negative', fila['negative'])\n",
    "    tupla_positive = ('positive', fila['positive'])\n",
    "    \n",
    "    # Si el emoji ya est√° en el diccionario, actualiza las listas existentes\n",
    "    if emoji in diccionario_caracteristicas:\n",
    "        diccionario_caracteristicas[emoji].extend([tupla_negative, tupla_positive])\n",
    "    else:\n",
    "        diccionario_caracteristicas[emoji] = [tupla_negative, tupla_positive]\n",
    "\n",
    "# Muestra el diccionario resultante\n",
    "nombre_archivo_pickle = 'lexicon_emoji.pkl'\n",
    "\n",
    "# Guarda el diccionario en el archivo pickle\n",
    "with open(nombre_archivo_pickle, 'wb') as archivo_pickle:\n",
    "    pickle.dump(diccionario_caracteristicas, archivo_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb7e92a",
   "metadata": {},
   "source": [
    "<h2> Generaci√≥n del corpus final con los acumulados positivos y negativos <h/2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19e9ea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Polarity                                      Title_Opinion   \n",
      "0             1  p√©simo lugar pensar dos vez antes de ir a este...  \\\n",
      "1             1  no vayas a lugar de Eddie cuatro de yo ir reci...   \n",
      "2             1  malo relaci√≥n calidad-precio seguir corto y si...   \n",
      "3             1  minusv√°lido ? ¬° no t√∫ alojser aqu√≠ ! al reserv...   \n",
      "4             1  ser uno porquerio no perder su tiempo no perde...   \n",
      "...         ...                                                ...   \n",
      "30207         5  Verdadera joya arquitect√≥nico ser uno construc...   \n",
      "30208         5  Rom√°ntico mucho al estilo de Romeo y Julieta s...   \n",
      "30209         5  parecer uno castillo Ideal para subir el escal...   \n",
      "30210         5  Imperdible ser imperdible , de ah√≠ poder ver m...   \n",
      "30211         5  mucho bonito visto no t√∫ poder ir de Guanajuat...   \n",
      "\n",
      "       _positive_  _negative_  _alegria_  _tristeza_  _enojo_  _repulsion_   \n",
      "0             0.0         0.0      0.792       1.129    1.392        0.630  \\\n",
      "1             0.0         0.0      0.000       0.993    1.122        0.598   \n",
      "2             0.0         0.0      0.000       0.265    0.561        0.000   \n",
      "3            24.0        22.0      1.223       0.000    0.561        0.000   \n",
      "4            12.0        11.0      1.026       2.516    0.864        0.000   \n",
      "...           ...         ...        ...         ...      ...          ...   \n",
      "30207         0.0         0.0      0.763       0.000    0.000        0.000   \n",
      "30208         0.0         0.0      0.630       0.000    0.000        0.000   \n",
      "30209         0.0         0.0      0.000       0.000    0.000        0.000   \n",
      "30210         0.0         0.0      1.263       0.000    0.000        0.000   \n",
      "30211         0.0         0.0      0.000       0.000    0.000        1.526   \n",
      "\n",
      "       _miedo_  _sorpresa_  acumuladopositivo  acumuladonegative  \n",
      "0        0.000       0.000              0.792              3.151  \n",
      "1        0.898       0.660              0.660              3.611  \n",
      "2        0.663       0.000              0.000              1.489  \n",
      "3        0.000       1.131             26.354             22.561  \n",
      "4        0.000       0.165             13.191             14.380  \n",
      "...        ...         ...                ...                ...  \n",
      "30207    0.000       0.000              0.763              0.000  \n",
      "30208    0.000       0.000              0.630              0.000  \n",
      "30209    0.000       0.000              0.000              0.000  \n",
      "30210    0.000       0.330              1.593              0.000  \n",
      "30211    0.000       0.000              0.000              1.526  \n",
      "\n",
      "[30212 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "def getSELFeatures(cadenas, lexicon_sel, lexicon_emoji):\n",
    "    # Funci√≥n para extraer caracter√≠sticas emocionales de las cadenas de texto\n",
    "    features = []\n",
    "    for cadena in cadenas:\n",
    "        # Inicializaci√≥n de valores de emociones y polaridades\n",
    "        valor_alegria = 0.0\n",
    "        valor_enojo = 0.0\n",
    "        valor_miedo = 0.0\n",
    "        valor_repulsion = 0.0\n",
    "        valor_sorpresa = 0.0\n",
    "        valor_tristeza = 0.0\n",
    "        valor_positivo = 0.0\n",
    "        valor_negativo = 0.0\n",
    "        \n",
    "        # Tokenizaci√≥n de la cadena de texto\n",
    "        cadena_palabras = re.split('\\s+', cadena)\n",
    "        dic = {}\n",
    "        \n",
    "        # Procesamiento de cada palabra en la cadena\n",
    "        for palabra in cadena_palabras:\n",
    "            if palabra in lexicon_sel:\n",
    "                # Si la palabra est√° en el lexicon_sel, se extraen caracter√≠sticas emocionales\n",
    "                caracteristicas = lexicon_sel[palabra]\n",
    "                for emocion, valor in caracteristicas:\n",
    "                    # Acumulaci√≥n de valores emocionales\n",
    "                    if emocion == 'Alegr√≠a':\n",
    "                        valor_alegria += float(valor)\n",
    "                    elif emocion == 'Tristeza':\n",
    "                        valor_tristeza += float(valor)\n",
    "                    elif emocion == 'Enojo':\n",
    "                        valor_enojo += float(valor)\n",
    "                    elif emocion == 'Repulsi√≥n':\n",
    "                        valor_repulsion += float(valor)\n",
    "                    elif emocion == 'Miedo':\n",
    "                        valor_miedo += float(valor)\n",
    "                    elif emocion == 'Sorpresa':\n",
    "                        valor_sorpresa += float(valor)\n",
    "\n",
    "            if palabra in lexicon_emoji:\n",
    "                # Si la palabra est√° en el lexicon_emoji, se extraen caracter√≠sticas de polaridad\n",
    "                caracteristicas2 = lexicon_emoji[palabra]\n",
    "                for polaridad, valor in caracteristicas2:\n",
    "                    # Acumulaci√≥n de valores de polaridad\n",
    "                    if polaridad == 'positive':\n",
    "                        valor_positivo = valor_positivo + float(valor)\n",
    "                    elif polaridad == 'negative':\n",
    "                        valor_negativo = valor_negativo + float(valor)\n",
    "\n",
    "        # Almacenamiento de valores acumulados en un diccionario\n",
    "        dic['_positive_'] = valor_positivo\n",
    "        dic['_negative_'] = valor_negativo\n",
    "        dic['_alegria_'] = valor_alegria\n",
    "        dic['_tristeza_'] = valor_tristeza\n",
    "        dic['_enojo_'] = valor_enojo\n",
    "        dic['_repulsion_'] = valor_repulsion\n",
    "        dic['_miedo_'] = valor_miedo\n",
    "        dic['_sorpresa_'] = valor_sorpresa\n",
    "\n",
    "        dic['acumuladopositivo'] = dic['_alegria_'] + dic['_sorpresa_'] + dic['_positive_']\n",
    "        dic['acumuladonegative'] = dic['_enojo_'] + dic['_miedo_'] + dic['_repulsion_'] + dic['_tristeza_'] + dic['_negative_']\n",
    "\n",
    "        # Agregaci√≥n del diccionario a la lista de caracter√≠sticas\n",
    "        features.append(dic)\n",
    "\n",
    "    return features\n",
    "\n",
    "def process_text(text):\n",
    "    # Funci√≥n para procesar texto usando spaCy\n",
    "    doc = nlp(text)\n",
    "    processed_text = ' '.join([token.lemma_ for token in doc if not token.is_stop])\n",
    "    return processed_text\n",
    "\n",
    "def process_row(row, lexicon_sel, lexicon_emoji):\n",
    "    # Funci√≥n para procesar una fila del DataFrame\n",
    "    polaridad = getSELFeatures([row['Title_Opinion']], lexicon_sel, lexicon_emoji)\n",
    "    return polaridad[0] if polaridad else None\n",
    "\n",
    "\"\"\"# Carga del modelo de procesamiento de lenguaje natural (spaCy)\n",
    "nlp = spacy.load(\"es_core_news_sm\")\"\"\"\n",
    "\n",
    "# Lectura del DataFrame desde un archivo pickle\n",
    "df = pd.read_pickle(\"df_tokenizacion_lematizacion_final.pkl\")\n",
    "\n",
    "# Carga de lexicones desde archivos pickle\n",
    "with open('lexicon_emoji.pkl', 'rb') as archivo_pickle:\n",
    "    lexicon_emoji = pickle.load(archivo_pickle)\n",
    "\n",
    "with open('lexicon_sel.pkl', 'rb') as archivo_pickle2:\n",
    "    lexicon_sel = pickle.load(archivo_pickle2)\n",
    "\n",
    "# Lista de emociones para iterar\n",
    "emotions = ['_positive_', '_negative_', '_alegria_', '_tristeza_', '_enojo_', '_repulsion_', '_miedo_', '_sorpresa_', 'acumuladopositivo', 'acumuladonegative']\n",
    "\n",
    "# Iteraci√≥n sobre las emociones para agregar columnas al DataFrame\n",
    "for emotion in emotions:\n",
    "    df[emotion] = df['Title_Opinion'].apply(lambda x: process_row({'Title_Opinion': x}, lexicon_sel, lexicon_emoji)[emotion] if process_row({'Title_Opinion': x}, lexicon_sel, lexicon_emoji) else None)\n",
    "\n",
    "# Eliminaci√≥n de la columna 'Attraction'\n",
    "df = df.drop(['Attraction'], axis=1)\n",
    "\n",
    "# Guardado del DataFrame resultante en un archivo pickle\n",
    "df.to_pickle('df_tokenizacion_lematizacion_emojis_final.pkl')\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952f28c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
