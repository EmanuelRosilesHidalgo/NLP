{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.calibration import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, make_scorer, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamaño de X_train: 24169 documentos\n",
      "Tamaño de X_test: 6043 documentos\n",
      "Tamaño de y_train: 24169 etiquetas\n",
      "Tamaño de y_test: 6043 etiquetas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer_frecuencia = CountVectorizer()\n",
    "vectorizer_binary = CountVectorizer(binary = True)\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "\n",
    "df_normalizacion = pd.read_pickle('df_tokenizacion_lematizacion_emociones.pkl')\n",
    "\n",
    "features = ['Title_Opinion','__alegria__','__tristeza__','__enojo__','__repulsion__','__miedo__','__sorpresa__','acumuladopositivo', 'acumuladonegative']\n",
    "numeric_features = ['acumuladopositivo', 'acumuladonegative','__alegria__','__tristeza__','__enojo__','__repulsion__','__miedo__','__sorpresa__']\n",
    "\n",
    "X = df_normalizacion[features]\n",
    "y = df_normalizacion['Polarity']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(f'\\nTamaño de X_train: {len(X_train)} documentos')\n",
    "print(f'Tamaño de X_test: {len(X_test)} documentos')\n",
    "print(f'Tamaño de y_train: {len(y_train)} etiquetas')\n",
    "print(f'Tamaño de y_test: {len(y_test)} etiquetas\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresion logistica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuaciones de Validación Cruzada (F1-score): [0.4514485  0.46663268 0.47502891 0.47197186 0.48131916]\n",
      "F1-score Promedio: 0.46928022217270715\n"
     ]
    }
   ],
   "source": [
    "# Vectorización de texto\n",
    "X_train_vectorizer = vectorizer_frecuencia.fit_transform(X_train['Title_Opinion'])\n",
    "X_test_vectorizer = vectorizer_frecuencia.transform(X_test['Title_Opinion'])\n",
    "\n",
    "# Combinar características numéricas y vectorizadas\n",
    "X_train_final = hstack([X_train_vectorizer, csr_matrix(X_train[numeric_features].values)])\n",
    "X_test_final = hstack([X_test_vectorizer, csr_matrix(X_test[numeric_features].values)])\n",
    "\n",
    "# Inicializar el clasificador\n",
    "clf_lr = LogisticRegression(max_iter = 10000)\n",
    "\n",
    "# Inicializar KFold con el número deseado de divisiones (folds)\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "\n",
    "# Definir la métrica F1-score para la validación cruzada\n",
    "scoring_metric = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Aplicar la validación cruzada y obtener las puntuaciones\n",
    "cv_scores = cross_val_score(clf_lr, X_train_final, y_train, cv = kf, scoring = scoring_metric)\n",
    "\n",
    "# Imprimir las puntuaciones de validación cruzada\n",
    "print(\"Puntuaciones de Validación Cruzada (F1-score):\", cv_scores)\n",
    "\n",
    "# Imprimir la puntuación media y la desviación estándar de las puntuaciones\n",
    "print(\"F1-score Promedio:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuaciones de Validación Cruzada (F1-score): [0.4469848  0.44746882 0.45709268 0.4634629  0.47633941]\n",
      "F1-score Promedio: 0.4582697210454281\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorizer = vectorizer_binary.fit_transform(X_train['Title_Opinion'])\n",
    "X_test_vectorizer = vectorizer_binary.transform(X_test['Title_Opinion'])\n",
    "\n",
    "X_train_final = hstack([X_train_vectorizer, csr_matrix(X_train[numeric_features].values)])\n",
    "X_test_final = hstack([X_test_vectorizer, csr_matrix(X_test[numeric_features].values)])\n",
    "\n",
    "clf_lr = LogisticRegression(max_iter = 10000)\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "\n",
    "scoring_metric = make_scorer(f1_score, average='macro')\n",
    "\n",
    "cv_scores = cross_val_score(clf_lr, X_train_final, y_train, cv = kf, scoring = scoring_metric)\n",
    "\n",
    "print(\"Puntuaciones de Validación Cruzada (F1-score):\", cv_scores)\n",
    "print(\"F1-score Promedio:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuaciones de Validación Cruzada (F1-score): [0.41454532 0.42188393 0.42508969 0.43379639 0.43699923]\n",
      "F1-score Promedio: 0.4264629110720174\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorizer = vectorizer_tfidf.fit_transform(X_train['Title_Opinion'])\n",
    "X_test_vectorizer = vectorizer_tfidf.transform(X_test['Title_Opinion'])\n",
    "\n",
    "X_train_final = hstack([X_train_vectorizer, csr_matrix(X_train[numeric_features].values)])\n",
    "X_test_final = hstack([X_test_vectorizer, csr_matrix(X_test[numeric_features].values)])\n",
    "\n",
    "clf_lr = LogisticRegression(max_iter = 10000)\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "\n",
    "scoring_metric = make_scorer(f1_score, average='macro')\n",
    "\n",
    "cv_scores = cross_val_score(clf_lr, X_train_final, y_train, cv = kf, scoring = scoring_metric)\n",
    "\n",
    "print(\"Puntuaciones de Validación Cruzada (F1-score):\", cv_scores)\n",
    "print(\"F1-score Promedio:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuaciones de Validación Cruzada (F1-score): [0.29238571 0.2730753  0.28131178 0.28585705 0.29416464]\n",
      "F1-score Promedio: 0.2853588963273918\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorizer = vectorizer_frecuencia.fit_transform(X_train['Title_Opinion'])\n",
    "X_test_vectorizer = vectorizer_frecuencia.transform(X_test['Title_Opinion'])\n",
    "\n",
    "X_train_final = hstack([X_train_vectorizer, csr_matrix(X_train[numeric_features].values)])\n",
    "X_test_final = hstack([X_test_vectorizer, csr_matrix(X_test[numeric_features].values)])\n",
    "\n",
    "clf_svm = SVC()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "scoring_metric = make_scorer(f1_score, average='macro')\n",
    "\n",
    "cv_scores = cross_val_score(clf_svm, X_train_final, y_train, cv=kf, scoring=scoring_metric)\n",
    "\n",
    "print(\"Puntuaciones de Validación Cruzada (F1-score):\", cv_scores)\n",
    "print(\"F1-score Promedio:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuaciones de Validación Cruzada (F1-score): [0.35799956 0.35803997 0.35765518 0.35823345 0.3705168 ]\n",
      "F1-score Promedio: 0.360488993544016\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorizer = vectorizer_binary.fit_transform(X_train['Title_Opinion'])\n",
    "X_test_vectorizer = vectorizer_binary.transform(X_test['Title_Opinion'])\n",
    "\n",
    "X_train_final = hstack([X_train_vectorizer, csr_matrix(X_train[numeric_features].values)])\n",
    "X_test_final = hstack([X_test_vectorizer, csr_matrix(X_test[numeric_features].values)])\n",
    "\n",
    "clf_svm = SVC()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "scoring_metric = make_scorer(f1_score, average='macro')\n",
    "\n",
    "cv_scores = cross_val_score(clf_svm, X_train_final, y_train, cv=kf, scoring=scoring_metric)\n",
    "\n",
    "print(\"Puntuaciones de Validación Cruzada (F1-score):\", cv_scores)\n",
    "print(\"F1-score Promedio:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuaciones de Validación Cruzada (F1-score): [0.23036897 0.23873725 0.22904402 0.23741561 0.2451755 ]\n",
      "F1-score Promedio: 0.23614827228001115\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorizer = vectorizer_tfidf.fit_transform(X_train['Title_Opinion'])\n",
    "X_test_vectorizer = vectorizer_tfidf.transform(X_test['Title_Opinion'])\n",
    "\n",
    "X_train_final = hstack([X_train_vectorizer, csr_matrix(X_train[numeric_features].values)])\n",
    "X_test_final = hstack([X_test_vectorizer, csr_matrix(X_test[numeric_features].values)])\n",
    "\n",
    "clf_svm = SVC()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "scoring_metric = make_scorer(f1_score, average='macro')\n",
    "\n",
    "cv_scores = cross_val_score(clf_svm, X_train_final, y_train, cv=kf, scoring=scoring_metric)\n",
    "\n",
    "print(\"Puntuaciones de Validación Cruzada (F1-score):\", cv_scores)\n",
    "print(\"F1-score Promedio:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuaciones de Validación Cruzada (F1-score): [0.43744381 0.45019299 0.44880103 0.4454098  0.47452307]\n",
      "F1-score Promedio: 0.4512741379595326\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorizer = vectorizer_frecuencia.fit_transform(X_train['Title_Opinion'])\n",
    "X_test_vectorizer = vectorizer_frecuencia.transform(X_test['Title_Opinion'])\n",
    "\n",
    "X_train_final = hstack([X_train_vectorizer, csr_matrix(X_train[numeric_features].values)])\n",
    "X_test_final = hstack([X_test_vectorizer, csr_matrix(X_test[numeric_features].values)])\n",
    "\n",
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=200)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "scoring_metric = make_scorer(f1_score, average='macro')\n",
    "\n",
    "cv_scores = cross_val_score(clf_mlp, X_train_final, y_train, cv=kf, scoring=scoring_metric)\n",
    "\n",
    "print(\"Puntuaciones de Validación Cruzada (F1-score):\", cv_scores)\n",
    "print(\"F1-score Promedio:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuaciones de Validación Cruzada (F1-score): [0.44332137 0.44945133 0.44443061 0.44495357 0.46089238]\n",
      "F1-score Promedio: 0.44860985228238703\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorizer = vectorizer_binary.fit_transform(X_train['Title_Opinion'])\n",
    "X_test_vectorizer = vectorizer_binary.transform(X_test['Title_Opinion'])\n",
    "\n",
    "X_train_final = hstack([X_train_vectorizer, csr_matrix(X_train[numeric_features].values)])\n",
    "X_test_final = hstack([X_test_vectorizer, csr_matrix(X_test[numeric_features].values)])\n",
    "\n",
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=200)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "scoring_metric = make_scorer(f1_score, average='macro')\n",
    "\n",
    "cv_scores = cross_val_score(clf_mlp, X_train_final, y_train, cv=kf, scoring=scoring_metric)\n",
    "\n",
    "print(\"Puntuaciones de Validación Cruzada (F1-score):\", cv_scores)\n",
    "print(\"F1-score Promedio:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuaciones de Validación Cruzada (F1-score): [0.42410442 0.46120458 0.44632765 0.45307946 0.44818173]\n",
      "F1-score Promedio: 0.44657956845187396\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorizer = vectorizer_tfidf.fit_transform(X_train['Title_Opinion'])\n",
    "X_test_vectorizer = vectorizer_tfidf.transform(X_test['Title_Opinion'])\n",
    "\n",
    "X_train_final = hstack([X_train_vectorizer, csr_matrix(X_train[numeric_features].values)])\n",
    "X_test_final = hstack([X_test_vectorizer, csr_matrix(X_test[numeric_features].values)])\n",
    "\n",
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=200)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "scoring_metric = make_scorer(f1_score, average='macro')\n",
    "\n",
    "cv_scores = cross_val_score(clf_mlp, X_train_final, y_train, cv=kf, scoring=scoring_metric)\n",
    "\n",
    "print(\"Puntuaciones de Validación Cruzada (F1-score):\", cv_scores)\n",
    "print(\"F1-score Promedio:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mejor resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  53   16   14    8   13]\n",
      " [  21   37   44   24   19]\n",
      " [  19   33  169  113   88]\n",
      " [   6   15   98  433  611]\n",
      " [   0    3   49  413 3744]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.51      0.52       104\n",
      "           2       0.36      0.26      0.30       145\n",
      "           3       0.45      0.40      0.42       422\n",
      "           4       0.44      0.37      0.40      1163\n",
      "           5       0.84      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.52      0.49      0.50      6043\n",
      "weighted avg       0.72      0.73      0.72      6043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorizer = vectorizer_frecuencia.fit_transform(X_train['Title_Opinion'])\n",
    "X_test_vectorizer = vectorizer_frecuencia.transform(X_test['Title_Opinion'])\n",
    "\n",
    "X_train_final = hstack([X_train_vectorizer, csr_matrix(X_train[numeric_features].values)])\n",
    "X_test_final = hstack([X_test_vectorizer, csr_matrix(X_test[numeric_features].values)])\n",
    "\n",
    "clf_lr = LogisticRegression(max_iter=10000)\n",
    "clf_lr.fit(X_train_final, y_train)\n",
    " \n",
    "y_pred = clf_lr.predict(X_test_final)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(conf_matrix)\n",
    "print(classification_rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
